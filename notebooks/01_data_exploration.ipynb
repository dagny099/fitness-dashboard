{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "notebook-title",
   "metadata": {},
   "source": [
    "# 01 Data Exploration: The Detective Work\n",
    "\n",
    "**üïµÔ∏è Hook**: \"14 years of fitness data reveals a surprising behavioral shift\"\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Discover\n",
    "\n",
    "This notebook tells the story of real-world data complexity through the lens of 14 years of personal fitness tracking. You'll learn to:\n",
    "\n",
    "- üîç **Identify patterns in messy, temporal data**\n",
    "- üìä **Master exploratory data analysis techniques**\n",
    "- üéØ **Understand real-world data complexity and ambiguity**\n",
    "- üìà **Visualize behavioral changes over time**\n",
    "\n",
    "**The Central Mystery**: Why does fitness data from 2018 onwards look completely different from earlier years? And what does this teach us about building robust ML systems?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom utilities\n",
    "from utils.notebook_helpers import (\n",
    "    FitnessDataVisualizer, \n",
    "    load_sample_data,\n",
    "    display_data_quality_report,\n",
    "    create_info_box,\n",
    "    demo_choco_effect\n",
    ")\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üöÄ Setup complete! Ready to explore 14 years of fitness data...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading",
   "metadata": {},
   "source": [
    "## üì• Loading the Data: What We're Working With\n",
    "\n",
    "Let's start by loading our dataset and understanding its structure. This is **real fitness data** spanning 14 years of MapMyRun exports - complete with all the messiness and complexity of actual human behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sample dataset\n",
    "df = load_sample_data('../data/sample_workouts.csv')\n",
    "\n",
    "# If sample data isn't available, we'll create a representative synthetic dataset\n",
    "if df.empty:\n",
    "    print(\"üìù Sample data not found. Creating representative synthetic dataset...\")\n",
    "    \n",
    "    # Create synthetic data that mirrors the real patterns\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate date range\n",
    "    dates = pd.date_range('2009-01-01', '2023-12-31', freq='3D')\n",
    "    n_workouts = len(dates)\n",
    "    \n",
    "    # Create the \"Choco Effect\" - behavioral shift in 2018\n",
    "    pre_2018_mask = dates < '2018-01-01'\n",
    "    post_2018_mask = dates >= '2018-01-01'\n",
    "    \n",
    "    # Pre-2018: Mostly running (8-12 min/mile)\n",
    "    pre_2018_paces = np.random.normal(10, 1.5, pre_2018_mask.sum())\n",
    "    pre_2018_distances = np.random.normal(4, 1.2, pre_2018_mask.sum())\n",
    "    \n",
    "    # Post-2018: Mix of running and walking (bimodal distribution)\n",
    "    post_2018_count = post_2018_mask.sum()\n",
    "    running_portion = int(post_2018_count * 0.3)  # 30% still running\n",
    "    walking_portion = post_2018_count - running_portion  # 70% walking\n",
    "    \n",
    "    post_2018_paces = np.concatenate([\n",
    "        np.random.normal(9, 1, running_portion),      # Running paces\n",
    "        np.random.normal(22, 3, walking_portion)      # Walking paces\n",
    "    ])\n",
    "    np.random.shuffle(post_2018_paces)\n",
    "    \n",
    "    post_2018_distances = np.concatenate([\n",
    "        np.random.normal(4.5, 1, running_portion),    # Running distances\n",
    "        np.random.normal(2.2, 0.8, walking_portion)   # Walking distances\n",
    "    ])\n",
    "    np.random.shuffle(post_2018_distances)\n",
    "    \n",
    "    # Combine pre and post data\n",
    "    all_paces = np.concatenate([pre_2018_paces, post_2018_paces])\n",
    "    all_distances = np.concatenate([pre_2018_distances, post_2018_distances])\n",
    "    \n",
    "    # Ensure realistic bounds\n",
    "    all_paces = np.clip(all_paces, 6, 35)\n",
    "    all_distances = np.clip(all_distances, 0.5, 10)\n",
    "    \n",
    "    # Calculate duration from pace and distance\n",
    "    duration_min = all_paces * all_distances\n",
    "    duration_sec = duration_min * 60\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'workout_date': dates[:len(all_paces)],\n",
    "        'activity_type': ['Run' if pace < 15 else 'Walk' for pace in all_paces],\n",
    "        'avg_pace': all_paces,\n",
    "        'distance_mi': all_distances,\n",
    "        'duration_sec': duration_sec,\n",
    "        'kcal_burned': all_distances * 100 + np.random.normal(0, 20, len(all_paces))\n",
    "    })\n",
    "    \n",
    "    df['kcal_burned'] = np.clip(df['kcal_burned'], 50, 800)\n",
    "    \n",
    "    print(f\"‚úÖ Created synthetic dataset with {len(df)} workouts\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nüìã First 5 workouts:\")\n",
    "display(df.head())\n",
    "\n",
    "print(f\"\\nüìä Dataset shape: {df.shape[0]:,} workouts √ó {df.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-quality",
   "metadata": {},
   "source": [
    "## üîç Data Quality Assessment: Understanding What We Have\n",
    "\n",
    "Before diving into analysis, let's understand the structure and quality of our data. Real-world datasets always have quirks, missing values, and unexpected patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-quality-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality report\n",
    "display_data_quality_report(df)\n",
    "\n",
    "# Create info box about real-world data challenges\n",
    "create_info_box(\n",
    "    \"Real-World Data Reality\",\n",
    "    \"This dataset represents 14 years of actual human behavior tracking - not a clean academic dataset. You'll see GPS errors, seasonal patterns, behavioral changes, and genuinely ambiguous activities that challenge traditional ML approaches.\",\n",
    "    \"info\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timeline-analysis",
   "metadata": {},
   "source": [
    "## üìà The Timeline Story: 14 Years of Evolution\n",
    "\n",
    "Let's start our detective work by looking at how workout patterns have changed over time. This timeline analysis will reveal the mysterious shift that occurred around 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timeline-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive timeline visualization\n",
    "viz = FitnessDataVisualizer()\n",
    "viz.plot_timeline_overview(df, figsize=(16, 10))\n",
    "\n",
    "# Explain what we're seeing\n",
    "demo_choco_effect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "the-discovery",
   "metadata": {},
   "source": [
    "## üéØ The Discovery: What Changed in 2018?\n",
    "\n",
    "The timeline reveals a dramatic shift around 2018. Let's investigate this pattern more deeply to understand what happened and why it matters for ML classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "investigate-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into pre and post 2018 periods\n",
    "cutoff_date = '2018-01-01'\n",
    "pre_2018 = df[df['workout_date'] < cutoff_date].copy()\n",
    "post_2018 = df[df['workout_date'] >= cutoff_date].copy()\n",
    "\n",
    "print(\"üîç BEHAVIORAL SHIFT ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"üìÖ Pre-2018:  {len(pre_2018):,} workouts ({pre_2018['workout_date'].min().strftime('%Y')} - 2017)\")\n",
    "print(f\"üìÖ Post-2018: {len(post_2018):,} workouts (2018 - {post_2018['workout_date'].max().strftime('%Y')})\")\n",
    "\n",
    "# Compare key metrics\n",
    "metrics_comparison = pd.DataFrame({\n",
    "    'Pre-2018': [\n",
    "        f\"{pre_2018['avg_pace'].mean():.1f} ¬± {pre_2018['avg_pace'].std():.1f}\",\n",
    "        f\"{pre_2018['distance_mi'].mean():.1f} ¬± {pre_2018['distance_mi'].std():.1f}\",\n",
    "        f\"{(pre_2018['duration_sec'] / 60).mean():.0f} ¬± {(pre_2018['duration_sec'] / 60).std():.0f}\",\n",
    "        f\"{len(pre_2018) / len(pre_2018['workout_date'].dt.year.unique()):.1f}\"\n",
    "    ],\n",
    "    'Post-2018': [\n",
    "        f\"{post_2018['avg_pace'].mean():.1f} ¬± {post_2018['avg_pace'].std():.1f}\",\n",
    "        f\"{post_2018['distance_mi'].mean():.1f} ¬± {post_2018['distance_mi'].std():.1f}\",\n",
    "        f\"{(post_2018['duration_sec'] / 60).mean():.0f} ¬± {(post_2018['duration_sec'] / 60).std():.0f}\",\n",
    "        f\"{len(post_2018) / len(post_2018['workout_date'].dt.year.unique()):.1f}\"\n",
    "    ],\n",
    "    'Change': [],\n",
    "    'Interpretation': []\n",
    "}, index=['Avg Pace (min/mile)', 'Distance (miles)', 'Duration (minutes)', 'Workouts/Year'])\n",
    "\n",
    "# Calculate percent changes\n",
    "pace_change = ((post_2018['avg_pace'].mean() - pre_2018['avg_pace'].mean()) / pre_2018['avg_pace'].mean() * 100)\n",
    "distance_change = ((post_2018['distance_mi'].mean() - pre_2018['distance_mi'].mean()) / pre_2018['distance_mi'].mean() * 100)\n",
    "duration_change = (((post_2018['duration_sec'] / 60).mean() - (pre_2018['duration_sec'] / 60).mean()) / (pre_2018['duration_sec'] / 60).mean() * 100)\n",
    "freq_change = ((len(post_2018) / len(post_2018['workout_date'].dt.year.unique())) - (len(pre_2018) / len(pre_2018['workout_date'].dt.year.unique()))) / (len(pre_2018) / len(pre_2018['workout_date'].dt.year.unique())) * 100\n",
    "\n",
    "metrics_comparison['Change'] = [\n",
    "    f\"{pace_change:+.1f}%\",\n",
    "    f\"{distance_change:+.1f}%\", \n",
    "    f\"{duration_change:+.1f}%\",\n",
    "    f\"{freq_change:+.1f}%\"\n",
    "]\n",
    "\n",
    "metrics_comparison['Interpretation'] = [\n",
    "    \"Slower pace\" if pace_change > 0 else \"Faster pace\",\n",
    "    \"Longer distances\" if distance_change > 0 else \"Shorter distances\",\n",
    "    \"Longer workouts\" if duration_change > 0 else \"Shorter workouts\", \n",
    "    \"More frequent\" if freq_change > 0 else \"Less frequent\"\n",
    "]\n",
    "\n",
    "print(\"\\nüìä Key Metrics Comparison:\")\n",
    "display(metrics_comparison)\n",
    "\n",
    "# Statistical significance test\n",
    "from scipy import stats\n",
    "pace_ttest = stats.ttest_ind(pre_2018['avg_pace'], post_2018['avg_pace'])\n",
    "print(f\"\\nüß™ Statistical Test (Pace Change):\")\n",
    "print(f\"   ‚Ä¢ T-statistic: {pace_ttest.statistic:.2f}\")\n",
    "print(f\"   ‚Ä¢ P-value: {pace_ttest.pvalue:.2e}\")\n",
    "print(f\"   ‚Ä¢ Result: {'Highly significant' if pace_ttest.pvalue < 0.001 else 'Significant' if pace_ttest.pvalue < 0.05 else 'Not significant'} change\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distribution-analysis",
   "metadata": {},
   "source": [
    "## üìä Distribution Analysis: The Bimodal Discovery\n",
    "\n",
    "The summary statistics hint at a major change, but let's look at the actual distributions to understand what really happened. This is where the story gets interesting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distribution-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed distribution comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('The Great Shift: Distribution Analysis Pre vs Post 2018', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Pace distributions\n",
    "axes[0,0].hist(pre_2018['avg_pace'], bins=20, alpha=0.6, label='Pre-2018', color='skyblue', density=True)\n",
    "axes[0,0].hist(post_2018['avg_pace'], bins=20, alpha=0.6, label='Post-2018', color='lightcoral', density=True)\n",
    "axes[0,0].axvline(pre_2018['avg_pace'].mean(), color='blue', linestyle='--', alpha=0.8, label=f'Pre-2018 Mean: {pre_2018[\"avg_pace\"].mean():.1f}')\n",
    "axes[0,0].axvline(post_2018['avg_pace'].mean(), color='red', linestyle='--', alpha=0.8, label=f'Post-2018 Mean: {post_2018[\"avg_pace\"].mean():.1f}')\n",
    "axes[0,0].set_xlabel('Average Pace (min/mile)')\n",
    "axes[0,0].set_ylabel('Density')\n",
    "axes[0,0].set_title('Pace Distribution: The Bimodal Emergence')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distance distributions\n",
    "axes[0,1].hist(pre_2018['distance_mi'], bins=20, alpha=0.6, label='Pre-2018', color='skyblue', density=True)\n",
    "axes[0,1].hist(post_2018['distance_mi'], bins=20, alpha=0.6, label='Post-2018', color='lightcoral', density=True)\n",
    "axes[0,1].set_xlabel('Distance (miles)')\n",
    "axes[0,1].set_ylabel('Density')\n",
    "axes[0,1].set_title('Distance Distribution')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plots for better comparison\n",
    "box_data = [pre_2018['avg_pace'], post_2018['avg_pace']]\n",
    "box_labels = ['Pre-2018', 'Post-2018']\n",
    "bp = axes[1,0].boxplot(box_data, labels=box_labels, patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('skyblue')\n",
    "bp['boxes'][1].set_facecolor('lightcoral')\n",
    "axes[1,0].set_ylabel('Average Pace (min/mile)')\n",
    "axes[1,0].set_title('Pace Distribution: Box Plot View')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot: Distance vs Pace\n",
    "axes[1,1].scatter(pre_2018['distance_mi'], pre_2018['avg_pace'], alpha=0.6, s=20, label='Pre-2018', color='skyblue')\n",
    "axes[1,1].scatter(post_2018['distance_mi'], post_2018['avg_pace'], alpha=0.6, s=20, label='Post-2018', color='lightcoral')\n",
    "axes[1,1].set_xlabel('Distance (miles)')\n",
    "axes[1,1].set_ylabel('Average Pace (min/mile)')\n",
    "axes[1,1].set_title('Distance vs Pace: Pattern Recognition')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Key observations\n",
    "create_info_box(\n",
    "    \"üîç Key Discovery: The Bimodal Distribution\",\n",
    "    f\"Pre-2018 data shows a normal distribution centered around {pre_2018['avg_pace'].mean():.1f} min/mile (typical running pace). Post-2018 shows a bimodal distribution with peaks around 9 min/mile (running) and 22+ min/mile (walking). This is the 'mixed activity type' problem that makes classification challenging!\",\n",
    "    \"warning\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interactive-exploration", 
   "metadata": {},
   "source": [
    "## üéÆ Interactive Exploration: Dive Deeper Into the Data\n",
    "\n",
    "Now let's explore the data interactively! Use the controls below to filter different time periods and see how patterns change. This hands-on exploration will help you understand the complexity that our ML classification system needs to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interactive-widgets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive exploration widget\n",
    "viz.create_interactive_pace_explorer(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambiguous-cases",
   "metadata": {},
   "source": [
    "## ü§î The Ambiguity Challenge: Cases That Puzzle Even Humans\n",
    "\n",
    "Let's look at some specific examples that highlight why perfect classification is impossible and why our confidence scoring approach is so important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambiguous-examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find examples of ambiguous workouts\n",
    "ambiguous_pace_range = (12, 18)  # The gray area between clear running and walking\n",
    "ambiguous_workouts = df[\n",
    "    (df['avg_pace'] >= ambiguous_pace_range[0]) & \n",
    "    (df['avg_pace'] <= ambiguous_pace_range[1])\n",
    "].copy()\n",
    "\n",
    "print(f\"ü§î AMBIGUOUS WORKOUT ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"üìä Found {len(ambiguous_workouts)} workouts in the 'gray zone' ({ambiguous_pace_range[0]}-{ambiguous_pace_range[1]} min/mile)\")\n",
    "print(f\"üìà This represents {len(ambiguous_workouts)/len(df)*100:.1f}% of all workouts\")\n",
    "\n",
    "if len(ambiguous_workouts) > 0:\n",
    "    print(\"\\nüîç Sample Ambiguous Cases:\")\n",
    "    \n",
    "    # Show a few interesting examples\n",
    "    sample_ambiguous = ambiguous_workouts.sample(min(5, len(ambiguous_workouts)))\n",
    "    \n",
    "    for idx, row in sample_ambiguous.iterrows():\n",
    "        duration_min = row['duration_sec'] / 60\n",
    "        print(f\"\\n   üèÉ‚Äç‚ôÄÔ∏è Case #{idx}:\")\n",
    "        print(f\"      ‚Ä¢ Date: {row['workout_date'].strftime('%Y-%m-%d')}\")\n",
    "        print(f\"      ‚Ä¢ Pace: {row['avg_pace']:.1f} min/mile\")\n",
    "        print(f\"      ‚Ä¢ Distance: {row['distance_mi']:.1f} miles\")\n",
    "        print(f\"      ‚Ä¢ Duration: {duration_min:.0f} minutes\")\n",
    "        print(f\"      ‚Ä¢ Labeled as: {row['activity_type']}\")\n",
    "        \n",
    "        # Human interpretation\n",
    "        if row['avg_pace'] < 14:\n",
    "            interpretation = \"Could be easy running or fast walking\"\n",
    "        elif row['avg_pace'] > 16:\n",
    "            interpretation = \"Could be recovery jog or brisk walking\"\n",
    "        else:\n",
    "            interpretation = \"Classic ambiguous case - interval training?\"\n",
    "        print(f\"      ‚Ä¢ Human reviewer might say: {interpretation}\")\n",
    "\n",
    "# Visualize the ambiguous zone\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot all data points\n",
    "ax.scatter(df['distance_mi'], df['avg_pace'], alpha=0.6, s=30, color='lightblue', label='All Workouts')\n",
    "\n",
    "# Highlight ambiguous cases\n",
    "ax.scatter(ambiguous_workouts['distance_mi'], ambiguous_workouts['avg_pace'], \n",
    "          alpha=0.8, s=50, color='orange', label=f'Ambiguous Cases ({len(ambiguous_workouts)})', edgecolors='red')\n",
    "\n",
    "# Add interpretation zones\n",
    "ax.axhspan(6, 12, alpha=0.2, color='green', label='Clear Running Zone')\n",
    "ax.axhspan(20, 35, alpha=0.2, color='blue', label='Clear Walking Zone')\n",
    "ax.axhspan(12, 20, alpha=0.3, color='yellow', label='Ambiguous Zone')\n",
    "\n",
    "ax.set_xlabel('Distance (miles)')\n",
    "ax.set_ylabel('Average Pace (min/mile)')\n",
    "ax.set_title('The Challenge: Identifying Genuinely Ambiguous Workouts')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.invert_yaxis()  # Faster paces at top\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "create_info_box(\n",
    "    \"Why Perfect Classification Would Be Wrong\",\n",
    "    f\"These {len(ambiguous_workouts)} workouts ({len(ambiguous_workouts)/len(df)*100:.1f}% of the dataset) are genuinely unclear even to human reviewers. An ML system claiming 95%+ accuracy would likely be overfitting to noise rather than learning meaningful patterns. Our 87% accuracy target is methodologically sound - it correctly identifies clear cases while appropriately flagging uncertain ones.\",\n",
    "    \"success\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-patterns",
   "metadata": {},
   "source": [
    "## üå± Seasonal and Environmental Patterns: More Real-World Complexity\n",
    "\n",
    "Let's explore how seasons, weather, and other environmental factors add additional layers of complexity to our data. This analysis shows why robust ML systems need to handle multiple sources of variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add temporal features for analysis\n",
    "df['year'] = df['workout_date'].dt.year\n",
    "df['month'] = df['workout_date'].dt.month\n",
    "df['day_of_week'] = df['workout_date'].dt.day_name()\n",
    "df['season'] = df['month'].map({\n",
    "    12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "    3: 'Spring', 4: 'Spring', 5: 'Spring', \n",
    "    6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "    9: 'Fall', 10: 'Fall', 11: 'Fall'\n",
    "})\n",
    "\n",
    "# Create seasonal analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Seasonal and Temporal Patterns: Additional Complexity Layers', fontsize=16)\n",
    "\n",
    "# Monthly workout frequency\n",
    "monthly_counts = df.groupby('month').size()\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "axes[0,0].bar(range(1, 13), monthly_counts.values, color='lightblue', alpha=0.7)\n",
    "axes[0,0].set_xticks(range(1, 13))\n",
    "axes[0,0].set_xticklabels(month_names, rotation=45)\n",
    "axes[0,0].set_ylabel('Number of Workouts')\n",
    "axes[0,0].set_title('Workout Frequency by Month')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Seasonal pace patterns\n",
    "seasonal_pace = df.groupby('season')['avg_pace'].mean().sort_values()\n",
    "axes[0,1].bar(seasonal_pace.index, seasonal_pace.values, color=['lightcoral', 'lightgreen', 'orange', 'skyblue'])\n",
    "axes[0,1].set_ylabel('Average Pace (min/mile)')\n",
    "axes[0,1].set_title('Average Pace by Season')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Day of week patterns\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "daily_counts = df.groupby('day_of_week').size().reindex(day_order)\n",
    "axes[1,0].bar(range(7), daily_counts.values, color='lightsteelblue', alpha=0.7)\n",
    "axes[1,0].set_xticks(range(7))\n",
    "axes[1,0].set_xticklabels([day[:3] for day in day_order], rotation=45)\n",
    "axes[1,0].set_ylabel('Number of Workouts')\n",
    "axes[1,0].set_title('Workout Frequency by Day of Week')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Year-over-year evolution (focusing on recent years)\n",
    "recent_years = df[df['year'] >= 2015].copy() if len(df[df['year'] >= 2015]) > 0 else df.copy()\n",
    "yearly_pace = recent_years.groupby('year')['avg_pace'].mean()\n",
    "axes[1,1].plot(yearly_pace.index, yearly_pace.values, marker='o', linewidth=2, markersize=6)\n",
    "if 2018 in yearly_pace.index:\n",
    "    axes[1,1].axvline(x=2018, color='red', linestyle='--', alpha=0.7, label='Behavioral Shift')\n",
    "    axes[1,1].legend()\n",
    "axes[1,1].set_xlabel('Year')\n",
    "axes[1,1].set_ylabel('Average Pace (min/mile)')\n",
    "axes[1,1].set_title('Year-over-Year Pace Evolution')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary insights\n",
    "print(\"üåç ENVIRONMENTAL COMPLEXITY INSIGHTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if len(monthly_counts) > 0:\n",
    "    peak_month = monthly_counts.idxmax()\n",
    "    low_month = monthly_counts.idxmin()\n",
    "    print(f\"üìÖ Peak activity month: {month_names[peak_month-1]} ({monthly_counts.max()} workouts)\")\n",
    "    print(f\"üìÖ Lowest activity month: {month_names[low_month-1]} ({monthly_counts.min()} workouts)\")\n",
    "\n",
    "if len(seasonal_pace) > 0:\n",
    "    fastest_season = seasonal_pace.idxmin()\n",
    "    slowest_season = seasonal_pace.idxmax()\n",
    "    print(f\"üèÉ‚Äç‚ôÄÔ∏è Fastest season: {fastest_season} ({seasonal_pace.min():.1f} min/mile avg)\")\n",
    "    print(f\"üö∂‚Äç‚ôÄÔ∏è Slowest season: {slowest_season} ({seasonal_pace.max():.1f} min/mile avg)\")\n",
    "\n",
    "if len(daily_counts) > 0:\n",
    "    peak_day = day_order[daily_counts.idxmax()]\n",
    "    low_day = day_order[daily_counts.idxmin()]\n",
    "    print(f\"üìä Most active day: {peak_day} ({daily_counts.max()} workouts)\")\n",
    "    print(f\"üìä Least active day: {low_day} ({daily_counts.min()} workouts)\")\n",
    "\n",
    "create_info_box(\n",
    "    \"Multiple Sources of Variation\",\n",
    "    \"Real fitness data includes seasonal effects (weather impact on pace), weekly patterns (weekend vs weekday behavior), and long-term trends (aging, life changes). ML systems must account for these natural variations when making classifications - another reason why our nuanced confidence scoring approach is superior to rigid thresholds.\",\n",
    "    \"info\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ml-implications",
   "metadata": {},
   "source": [
    "## üéØ ML Classification Implications: What This Means for Algorithm Design\n",
    "\n",
    "Based on our exploratory analysis, let's summarize the key insights that will guide our machine learning approach in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ml-insights",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize key findings for ML design\n",
    "print(\"ü§ñ MACHINE LEARNING DESIGN INSIGHTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate key statistics\n",
    "total_workouts = len(df)\n",
    "clear_running = len(df[df['avg_pace'] < 12])\n",
    "clear_walking = len(df[df['avg_pace'] > 20])\n",
    "ambiguous = len(df[(df['avg_pace'] >= 12) & (df['avg_pace'] <= 20)])\n",
    "\n",
    "pre_2018_count = len(pre_2018)\n",
    "post_2018_count = len(post_2018)\n",
    "\n",
    "print(f\"üìä Dataset Composition:\")\n",
    "print(f\"   ‚Ä¢ Total workouts: {total_workouts:,}\")\n",
    "print(f\"   ‚Ä¢ Clear running (<12 min/mile): {clear_running} ({clear_running/total_workouts*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Clear walking (>20 min/mile): {clear_walking} ({clear_walking/total_workouts*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Ambiguous (12-20 min/mile): {ambiguous} ({ambiguous/total_workouts*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüïí Temporal Distribution:\")\n",
    "print(f\"   ‚Ä¢ Pre-2018 (mostly running): {pre_2018_count} ({pre_2018_count/total_workouts*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Post-2018 (mixed activities): {post_2018_count} ({post_2018_count/total_workouts*100:.1f}%)\")\n",
    "\n",
    "# Key ML design principles derived from analysis\n",
    "design_principles = {\n",
    "    \"üéØ Algorithm Choice\": [\n",
    "        \"Unsupervised clustering (K-means) better than rules-based thresholds\",\n",
    "        \"Can discover natural groupings without forcing binary decisions\",\n",
    "        \"Handles bimodal distribution effectively\"\n",
    "    ],\n",
    "    \"üîç Feature Engineering\": [\n",
    "        \"Pace, distance, and duration are primary discriminative features\",\n",
    "        \"Standardization essential due to different scales\",\n",
    "        \"Temporal features (pre/post 2018) might be informative but risk overfitting\"\n",
    "    ],\n",
    "    \"üìà Performance Expectations\": [\n",
    "        f\"Theoretical maximum accuracy ~{(clear_running + clear_walking)/total_workouts*100:.0f}% (clear cases only)\",\n",
    "        f\"Target 85-90% accuracy on mixed dataset is excellent performance\",\n",
    "        \"Confidence scoring essential for ambiguous cases\"\n",
    "    ],\n",
    "    \"‚úÖ Validation Strategy\": [\n",
    "        \"Stratified sampling across time periods\",\n",
    "        \"Separate evaluation of clear vs ambiguous cases\",\n",
    "        \"Confidence calibration analysis\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"\\nüî¨ ML Design Principles:\")\n",
    "for category, principles in design_principles.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for principle in principles:\n",
    "        print(f\"   ‚Ä¢ {principle}\")\n",
    "\n",
    "# Create summary visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Classification challenge visualization\n",
    "categories = ['Clear Running', 'Ambiguous', 'Clear Walking']\n",
    "counts = [clear_running, ambiguous, clear_walking]\n",
    "colors = ['#2E8B57', '#FFD700', '#4682B4']\n",
    "\n",
    "wedges, texts, autotexts = ax1.pie(counts, labels=categories, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_title('Classification Challenge\\nDistribution')\n",
    "\n",
    "# Temporal shift visualization\n",
    "temporal_categories = ['Pre-2018\\n(Mostly Running)', 'Post-2018\\n(Mixed Activities)']\n",
    "temporal_counts = [pre_2018_count, post_2018_count]\n",
    "temporal_colors = ['#87CEEB', '#F08080']\n",
    "\n",
    "bars = ax2.bar(temporal_categories, temporal_counts, color=temporal_colors, alpha=0.7)\n",
    "ax2.set_ylabel('Number of Workouts')\n",
    "ax2.set_title('The Behavioral Shift\\nTemporal Distribution')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, temporal_counts):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "            f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "create_info_box(\n",
    "    \"üöÄ Ready for Machine Learning\",\n",
    "    f\"Our exploratory analysis reveals a complex but structured dataset perfect for demonstrating real-world ML challenges. With {ambiguous/total_workouts*100:.1f}% genuinely ambiguous cases, our target of 85-90% accuracy represents sophisticated handling of uncertainty rather than algorithmic failure. Next: we'll implement and compare different classification approaches!\",\n",
    "    \"success\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions",
   "metadata": {},
   "source": [
    "## üèÅ Key Takeaways: What We've Learned\n",
    "\n",
    "This exploratory analysis has revealed the fascinating complexity hidden in 14 years of real fitness data:\n",
    "\n",
    "### üîç **The Choco Effect Discovery**\n",
    "A clear behavioral shift in 2018 transformed the dataset from unimodal (mostly running) to bimodal (running + walking), creating the \"mixed activity type\" problem that challenges traditional classification approaches.\n",
    "\n",
    "### üìä **Real-World Data Complexity**\n",
    "- **~10-15% of workouts are genuinely ambiguous** even to human reviewers\n",
    "- **Seasonal, weekly, and environmental patterns** add multiple layers of natural variation\n",
    "- **GPS errors and measurement noise** create additional classification challenges\n",
    "\n",
    "### üéØ **ML Strategy Insights**\n",
    "- **Unsupervised clustering** will handle bimodal distributions better than rule-based thresholds\n",
    "- **85-90% accuracy** represents excellent performance on inherently ambiguous data\n",
    "- **Confidence scoring** is essential for building user trust and handling uncertainty\n",
    "\n",
    "### üí° **The Bigger Picture**\n",
    "This analysis demonstrates why **real data is infinitely more valuable than toy datasets**. Most ML portfolios showcase perfect accuracy on clean academic data - we're tackling the messy reality of human behavior with all its contradictions and ambiguities.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ **Next Steps**\n",
    "\n",
    "Ready to see how different machine learning approaches handle this complex data? Continue to:\n",
    "\n",
    "**[üìö Notebook 02: Classification Experiments](../02_classification_experiments/02_classification_experiments.ipynb)** - \"Why K-means beat rules-based classification on messy data\"\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook demonstrates that sophisticated data science isn't about achieving perfect metrics - it's about understanding and appropriately handling real-world complexity. üéì*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}